# Chains

Pipelines are definition of the data process flow. In LangChain terms, [Chains](https://docs.langchain.com/docs/components/chains/) and [Agents](https://docs.langchain.com/docs/components/agents/) fall into this structure.

Pipelines are composable and reusable. It supports DAG (Directed Acyclic Graph) style composability to let you
concatenate sequence of data flow across the system.

## Define chains

Chains are defined in the `/chains` directory.
The convection is `chains/<name>/main.py`.

Example:

```
chains/
  question-answering/
    latest.py
  agent-based-chat.py/
    latest.py
```

## Chain dependencies

Often chains have dependencies on [vector stores](../docs/database.md#vector-stores).
You must define the dependencies via `@dependency` directives.

Example:

```python
@dependency("vector_store", "index_documents_0_0_1")
def retrive_from_vector_store
  # Your business logic
end
```

## Chain versioning

To deploy a chain to production, it must be versioned,
so that you can ensure backward-compabiility, reproducibility, and traceability.

To compile a chain, you can execute `langapp compile` CLI command.
This automatically generates the following files:

Example:

```
chains/
  question-answering/
    latest.py           # Modifiable, the latest version. This should NOT be used in production.
    0_0_1.py            # Immutable, it's complied from the latest version. This can be used in production.
    0_0_2.py            # Immutable, it's complied from the latest version. This can be used in production.
```

Example of a versioned file:

```
# 0_0_1.py

"""
This file is auto-generated by LangApp. DO NOT EDIT.
"""

....
```

Chain versioning follows [Semantic Versioning](../docs/glossary.md#semantic-versioning).

Each pipeline is compiled into a docker image to ensure it's immutability.
This way we can A/B testing different pipelines and roll back to the previous version easily.

## Test chains locally

To test a chain, you can execute `langapp execute chain --version <version>`.

## Agents

[“Agents”](https://docs.langchain.com/docs/components/agents/) are essentially subset of the chains, therefore it's provided by the same interface.

## Define pipelines

Pipelines are defined in YAML/JSON format.

```
pipelines/
  question-answering/
    main.py
    spec.yaml
  pipeline.yaml
```

pipeline.yaml
  
```
version: 0.0.1
sha: <git-commit-sha>
steps:
  - name: question-answering
    version: 0.0.1
    sha: <git-commit-sha>
    inputs:
      - name: question
        type: text
      - name: context
        type: text
    outputs:
      - name: answer
        type: text
```

### Pipeline APIs

[Chains](https://docs.langchain.com/docs/components/chains/) is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.
Applications provide the interfaces for interacting with [chains](https://docs.langchain.com/docs/components/chains/) via HTTP requests.

Path formats:

```
POST /api/v1/pipelines/<name> ... Execute a chain
POST /api/v1/pipelines/<name>/feedback ... Post feedback for the chain
```

[“Agents”](https://docs.langchain.com/docs/components/agents/) are essentially subclasses of the chains, therefore it's provided by the same interface.

For more information, see [Developments](docs/developments.md).

### Versioning and composability

Pipelines, which is the definition of data flow, are versioned and immutable.
LangApp compiles the code into a docker image and tag it with a version number.
You can later strategies how these versions should be handled on production, such as,
deploying to the latest version, canary deployment, A/B testing, etc.

Pipelines are defined in the YAML file to make it composable and reusable.

### Import Jupyter Notebook to Pipeline

You can import [Jupyter notebook](https://jupyter.org/) into a pipeline file.
This is useful when you already have a working demo in [Google Colab](https://colab.research.google.com/),
and want to deploy it to production.

### Export Pipeline to Jupyter Notebook

You can export a pipeline to a jupyter notebook.
This is useful when you want to go back to the experimental and debugging phase to improve the pipeline.
For example, testing different LLMs, prompts, document splitting, chunking, annotations,
for finding a potential optimization for your pipelines.

Later, you can [import the notebook back to the pipeline file](#import-jupyter-notebook-to-pipeline) again.
