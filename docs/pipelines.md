# Pipelines

Pipelines are definition of the data process flow. In LangChain terms, [Chains](https://docs.langchain.com/docs/components/chains/) and [Agents](https://docs.langchain.com/docs/components/agents/) fall into this structure.

Pipelines are composable and reusable. It supports DAG (Directed Acyclic Graph) style composability to let you
concatenate sequence of data flow across the system.

## Define chains

Chains are defined in the `/chains` directory.
The convection is `chains/<name>/main.py`.

Example:

```
chains/
  question-answering/
    latest.py
  agent-based-chat.py/
    latest.py
```

## Chain dependencies

Often chains have dependencies on [vector stores](../docs/database.md#vector-stores).
You must define the dependencies via `@dependency` directives.

Example:

```python
@dependency("vector_store", "index_documents_0_0_1")
def retrive_from_vector_store
  # Your business logic
end
```

## Chain versioning

To deploy a chain to production, it must be versioned,
so that you can ensure backward-compabiility, reproducibility, and traceability.

To compile a chain, you can execute `langapp compile` CLI command.
This automatically generates the following files:

Example:

```
chains/
  question-answering/
    latest.py           # Modifiable, the latest version. This should NOT be used in production.
    0_0_1.py            # Immutable, it's complied from the latest version. This can be used in production.
    0_0_2.py            # Immutable, it's complied from the latest version. This can be used in production.
```

Example of a versioned file:

```
# 0_0_1.py

"""
This file is auto-generated by LangApp. DO NOT EDIT.
"""

....
```

Chain versioning follows [Semantic Versioning](../docs/glossary.md#semantic-versioning).

Each pipeline is compiled into a docker image to ensure it's immutability.
This way we can A/B testing different pipelines and roll back to the previous version easily.

## Test chains locally

To test a chain, you can execute `langapp execute chain --version <version>`.

## Agents

[“Agents”](https://docs.langchain.com/docs/components/agents/) are essentially subset of the chains, therefore it's provided by the same interface.

## Define pipelines

Pipelines are defined in YAML/JSON format.

```
pipelines/
  question-answering/
    main.py
    spec.yaml
  pipeline.yaml
```

pipeline.yaml
  
```
version: 0.0.1
sha: <git-commit-sha>
steps:
  - name: question-answering
    version: 0.0.1
    sha: <git-commit-sha>
    inputs:
      - name: question
        type: text
      - name: context
        type: text
    outputs:
      - name: answer
        type: text
```

### Pipeline APIs

[Chains](https://docs.langchain.com/docs/components/chains/) is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.
Applications provide the interfaces for interacting with [chains](https://docs.langchain.com/docs/components/chains/) via HTTP requests.

Path formats:

```
POST /api/v1/pipelines/<name> ... Execute a chain
POST /api/v1/pipelines/<name>/feedback ... Post feedback for the chain
```

[“Agents”](https://docs.langchain.com/docs/components/agents/) are essentially subclasses of the chains, therefore it's provided by the same interface.

For more information, see [Developments](docs/developments.md).

### Versioning and composability

Pipelines, which is the definition of data flow, are versioned and immutable.
LangApp compiles the code into a docker image and tag it with a version number.
You can later strategies how these versions should be handled on production, such as,
deploying to the latest version, canary deployment, A/B testing, etc.

Pipelines are defined in the YAML file to make it composable and reusable.

### Import Jupyter Notebook to Pipeline

You can import [Jupyter notebook](https://jupyter.org/) into a pipeline file.
This is useful when you already have a working demo in [Google Colab](https://colab.research.google.com/),
and want to deploy it to production.

### Export Pipeline to Jupyter Notebook

You can export a pipeline to a jupyter notebook.
This is useful when you want to go back to the experimental and debugging phase to improve the pipeline.
For example, testing different LLMs, prompts, document splitting, chunking, annotations,
for finding a potential optimization for your pipelines.

Later, you can [import the notebook back to the pipeline file](#import-jupyter-notebook-to-pipeline) again.

### BasePipeline

```python
class BasePipeline:
    def __init__(self):
        self.steps = []
    
    def step(self, func):
        self.steps.append(func)
        return func
    
    def execute(self, inputs):
        outputs = inputs
        for step in self.steps:
            outputs = step(outputs)
        return outputs
```

### Customize a pipeline

```python
class MyPipeline(BasePipeline):
    @pipeline.step
    def load_document(inputs):
        # Your business logic
        return outputs

    @pipeline.step
    def parse_document(inputs):
        # Your business logic
        return outputs
```

### Execute a pipeline

Execute a single pipeline:

```python
pipeline = MyPipeline.new(...)
result = pipeline.execute(inputs)
```

### Downstream pipeline

You can pass the output generated in the upstream pipeline to a downstream pipeline and return the final result to the client.
This is useful if you want to run multiple pipelines at once and combine the results or perform A/B testing for monitoring the difference of results.

Here is an example:

```python
# Create pipeline instances
pipeline_a = MyPipelineA.new(...)
pipeline_b = MyPipelineB.new(...)
pipeline_c = MyPipelineC.new(...)
pipeline_d = MyPipelineD.new(...)

# Pipeline-A is the upstream pipeline and sets Pipeline-B and Pipeline-C as the downstream pipelines.
pipeline_a.set_downstream_pipeline(pipeline_b)
pipeline_a.set_downstream_pipeline(pipeline_c)

# Pipeline-D is the finalize pipeline and accumulates the results from Pipeline-B and Pipeline-C.
pipeline_b.set_downstream_pipeline(pipeline_d, async=true)
pipeline_c.set_downstream_pipeline(pipeline_d, async=true)

# 1. Executes the Pipeline-A
# 1. Executes the Pipeline-B and Pipeline-C in parallel
# 1. Waits for both Pipeline-B and Pipeline-C has finished, and pass the results to the Pipeline-D
pipeline_a.execute(inputs)
```

### Asynchronous pipeline

You can execute a pipeline asynchronously in a case it takes a long time to finish.
Once the pipeline is enqueued, background worker executes the 

To illustrate:

1. A client and server establishes [websocket](https://en.wikipedia.org/wiki/WebSocket) for bi-directional connection.
1. A client requestes to LangApp to execute a pipeline. LangApp enqueues the execution request and return "enqueued" response to the client.
1. Once the background execution is finished, LangApp posts the result to the client.

Even if the websocket is disconnected, the result is persisted in the database, therefore clients can fetch the response later.
